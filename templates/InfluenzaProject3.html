{% extends "base.html" %}

{% block title %}Influenza Forecasting{% endblock %}

{% block head %}
{{ super() }}
<link rel="stylesheet" href="https://cdn.pydata.org/bokeh/release/bokeh-0.12.13.css" type="text/css" />
<link rel="stylesheet" type="text/css" href="{{ url_for('static', filename='css/fpslide.css') }}">
<link rel="stylesheet" type="text/css" href="{{ url_for('static', filename='css/pageFormattingInfluenza.css') }}">
{% endblock %}

{% block page_content %}

<h1>Influenza Forecasting</h1>

<div class="divContentTable">
    <ol>
        <li class="notCurrentLiContentTable"> <a class="aContentTable" href="{{ url_for('render_influenza_project1') }}">Overview</a> </li>
        <li class="notCurrentLiContentTable"> <a class="aContentTable" href="{{ url_for('render_influenza_project2') }}">Influenza Wave Statistics</a> </li>
        <li> <a class="aContentTable" href="#">Forecasting as a Classification Problem</a> </li>
    </ol>
</div>

<p>
    <p class="pSmallHeading">
        Forecasting as a machine learning problem:
    </p>
    The Forecasting problem can be framed at least in two ways. Either as a regression problem that is to say the exact
    number of influenza infections is predicted x weeks in advance. Or the problem can be framed as a
    classification problem in which the prediction is whether a certain threshold is crossed in the week x. For instance
    are more than 7.0 cases reported per 100 000 inhabitants in five weeks from now. It turns out that classifying
    whether a certain threshold of reported influenza infections will be crossed can be performed more reliably when
    compared to the regression problem. Although it would be nice to predict the exact number of infections the
    classification on the one hand provides sufficient information to predict the start, end and intensity which is
    crucial. And more importantly it turned out that only framing the problem as a classification problem made it
    accessible for longer forecasting periods. This is crucial for having enough time to prepare for an influenza wave.
    <br>
    <br>
    After gaining this insight two thresholds were chosen, namely 0.8 and 7.0 reported cases of influenza per 100 000
    inhabitants. The threshold 0.8 was chosen since it is relatively likely that an influenza wave starts once this
    threshold is crossed. On the other hand once the threshold of 7.0 is crossed the wave is relatively severe.
    <br>
    <br>
    Actually it would be more accurate to refer to the forecast as a set of classification problems. Since not one
    classification model is trained to make predictions 1,2, ..., 15 weeks in advance but for each of the fifteen
    forecasting periods two models are trained. One for the threshold 0.8 and one for the threshold 7.0. Therefore in
    sum 30 models are trained.
    <br>
    <br>
</p>

<p>
    <p class="pSmallHeading"> What is the features space? </p>
    As mentioned on the first page the model features are the weekly number of <a href="https://survstat.rki.de/Content/Query/Create.aspx">
    reported influenza infections</a> and <a href="https://www.google.org/flutrends/about/">Google Flu Trends Scores</a>
    reflecting the frequency of influenza related search queries on a state level and for Germany as a whole normalized
    by the number of inhabitants. As a side note it should be mentioned that including
    <a href="ftp://ftp-cdc.dwd.de/pub/CDC/observations_germany/climate/daily/kl/historical/">weather features</a> like
    temperature, humidity and precipitation turned out to not improve the predictive power of the model. Which is
    interesting when compared to the visual impression of the following plot. After displaying the temperature feature
    by clicking on "Temperature" in the legend a first guess form looking at the plot could be that
    the temperature has significant predictive power with respect to the reported influenza cases. Inferring from the
    plot a strong relation between the google trends score and the influenza infections could be expected. Except for
    the year 2009/2010 the google trends score seems to have a repetitive patter. Comparing the influenza infections on
    a state level with the infection numbers for Germany as a whole shows a strong resemblance with some occasional
    deviations. To <b>hide/unhide</b> a particular feature just <b>click on the feature name</b> in the legend.
</p>

<div class="wrapperSelectFigure">
    <div>
        <select class="selectDropDown" id="featuresSingleSelect">
            {% for item in stateSequence -%}
            <option value={{ item }}>{{ item }}</option>
            {%- endfor %}
        </select>
    </div>
    <div id="featureFigureDiv" class="singleSelectAbove">
        <div id="placeForFeaturesFigure">
            {{ script_features|safe }}
            {{ div_features|safe }}
        </div>
    </div>
</div>

<p>
    Since the weather features did not contribute to the forecast performance only the reported influenza infections and
    the Google Flu trends data are used as model features in the following way.
    <br>
    <br>
    The above data is available for the period from 2005 until 2015 and the features are constructed using a so called
    rolling window. This means that for each state and each combination of year and week of the year (e.g. state: Bayern,
    year: 2012, week: 33) the influenza and google trends scores of the past ten weeks for the specific state and for
    Germany as a whole are used as features to predict 1,2, ... 15 weeks in advance for the specific state.

    So, an example of a feature vector would be the following. If the current state is Bayern, the current year is 2012,
    and the current week of the year is 33 then this would lead to the following feature vector with 40 dimensions.
    For the weeks 24, 25, ... 31, 32, 33 of the year 2012 the reported influenza numbers and the google flu trends
    scores for Bayern and for Germany as a whole are included.
    <br>
    <br>
</p>

<p>
    <p class="pSmallHeading">
        How were the model and the features selected?
    </p>
    The first question that arose was whether it would be better to frame the problem as a regression or classification
    problem. As mention previously although many different combinations of regressors and features were checked none
    provided satisfying predictions for forecasting periods longer than 2 weeks. As in the classification including
    weather features and features like week of the year and state (one hot encoded) did not improve the model
    performance. Therefore the problem was framed as a classification problem. After initial spot checking of different
    classifiers it became clear that the classification approach yielded satisfying results as we will see in the next
    paragraph. The features were selected by backward elimination. The resulting features were stated in the previous
    paragraph. The best performing model was a fully connected neural net. The interested reader is referred to this
    github repository containing the code for the model evaluation.
    <br>
    <br>
</p>

<p>
    <p class="pSmallHeading">How did the model perform?</p>
    First, it should be briefly mentioned that this classification problem is imbalanced. As the above feature plot
    confirms the percentage of weeks crossing the threshold of 0.8 respectively 7.0 infections per 100 000 inhabitants
    is approximately 17% respectively 4%. To counter the imbalance the training set was enlarged by copying the rare
    classes such that a ratio of approximately 1:1 is reached. Therefore enough weight is on the rare class when the
    influence of a false prediction on the overall loss is calculated during training. Still the following metric
    visualizations will show that the large imbalance in case of the classification associated with the threshold 7.0
    seems to lead to significantly poorer predictions.
    <br>
    <br>
    The following plot visualizes the results of the cross-validation with respect to different metrics. Each fold of the
    cross-validation is associated with one year. For instance to predict the reported influenza infections for 2005
    (more precisely the cold weather season 2005/2006) the model is trained on the interval from week 25 of 2006 until
    week 24 of 2015 (excluding 2009). One model per forecasting distance is trained on all sixteen states. The main
    reason for training one model on all sixteen states is the relatively small number of wave samples. The following
    figure shows the performance of the model as seen in the animation on the first page with respect to accuracy,
    precision, recall, F2 score, ROC AUC and Log Loss. The metrics can simply be selected via the select drop down menu.
    For each forecasting distance from one to fifteen weeks the bars show the metric score of all cross-validation folds
    on the validation sets. The x marks the metric score on the validation set of a specific year. Hovering over the x
    shows the associated validation year.

</p>

<div class="wrapperSelectFigure">
    <div>
        <select class="selectDropDown" id="metricSingleSelect">
            {% for item in metricSequence -%}
            <option value={{ item }}>{{ item }}</option>
            {%- endfor %}
        </select>
    </div>
    <div id="metricFigureDiv" class="singleSelectAbove">
        <div id="placeForMetricFigure">
            {{ script_metric|safe }}
            {{ div_metric|safe }}
        </div>
    </div>
</div>

<p>
    Almost irrespective of the metric the above plot shows a relatively sharp performance decrease in the first
    four weeks. As mentioned above this effect is stronger for the more imbalanced classification problem associated
    with the threshold 7.0. After the four weeks the performance decrease slows down. The following visualizations of
    the confusion matrices with respect to the threshold 0.8 show that the longer the forecasting period the
    more false positives are predicted and the less false negatives. In absolute terms the false positive predictions
    outweigh the false negative predictions. This improves the recall and worsens the precision. The following
    visualizations follow the convention that class 0 corresponds to predicting that the threshold is not crossed and
    class 1 corresponds to predicting that the threshold is crossed.
</p>

<div id="confMatThr1FigureDiv" class="metricsFigureDiv">
            {{ script_conf_mat_thr1|safe }}
            {{ div_conf_mat_thr1|safe }}
</div>

<p>
    The confusion matrix above respectively below shows the confusion matrices associated with the threshold 0.8
    respectively threshold 7.0. In both cases it can be observed that the false positive increases significantly
    when predicting more weeks in advance. Above the false positive rate changes from roughly 0.16 in week 1 to
    0.5 in week 15. Below the false positive rate starts at approximately 0.40 and reaches 0.83 in week 15. Thus only 17
    percent of the predicted positives are actually positives. The recall benefits from a looser positive
    prediction behaviour as can be observed for both thresholds especially for longer forecasting distances.
</p>

<div id="confMatThr2FigureDiv" class="metricsFigureDiv">
            {{ script_conf_mat_thr2|safe }}
            {{ div_conf_mat_thr2|safe }}
</div>


<p>
    <br>
    <br>
    <p class="pSmallHeading">
        Conclusion:
    </p>
    As mentioned above for the threshold 0.8 the model performs quite reliably in the first three weeks with a
    good balance between recall, precision and false positive rate. For the threshold 7.0 the model has a satisfying
    recall but lacks good precision. Therefore the model predicts severe influenza waves more loosely and is less likely
    to miss out to forecast a week with a high number of influenza infections. For longer forecasting distances the predictions
    associated to the threshold 0.8 retain a good recall. The precision decreases but still for a forecasting period
    of 15 weeks the precision stays above 0.5. In case of the predictions associated with the threshold 7.0 a similar
    behavior can be observed although on a significantly lower performance level. The precision for longer forecasting
    falls below 0.2. This setup is desirable in scenarios where it is important to spot a wave reliably in advance and
    occasional false positive predictions are acceptable. Depending on the real world needs the probability threshold to
    label a class as positive could be modified to either favor recall by decreasing the threshold below 0.5 or to favor
    precision by increasing the threshold above 0.5.
</p>

<p>
    This implementation can be seen as a proof of concept for the initially stated goal of forecasting the reported number
    of influenza cases. This relatively simple approach and model already yield satisfying results. Next steps to further
    improve the model could be to include more features. For instance first differences or the weekly difference between
    the influenza numbers on a state level and for Germany as a whole could be included. Applying a panel approach could
    also be an interesting exercise. Although this would reduce the number of training examples even further by a factor
    of 1/16. The interested reader is referred to the github repository where the code for the data preparation, the
    regression and classification setup from training to selection to visualization can be found. In addition, the
    repository contains a jupyter notebook which provides some explorative steps. And finally this repository contains
    the code for this homepage.
</p>

<div class="pagination1">
  <a href="{{ url_for('render_influenza_project2') }}">&laquo;</a>
  <a href="{{ url_for('render_influenza_project1') }}">1</a>
    <a href="{{ url_for('render_influenza_project2') }}">2</a>
  <a class="active" href="#">3</a>
  <a href="#">&raquo;</a>
</div>

{% endblock %}

{% block scripts %}
{{ super() }}
<script src="//cdn.pydata.org/bokeh/release/bokeh-0.12.13.min.js"></script>
<script src="//cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.13.min.js"></script>
<script src="https://d3js.org/d3.v4.min.js"></script>

<script src="{{ url_for('static', filename='js/fpslide.js') }}"></script>
<script src="{{ url_for('static', filename='js/ajaxscripts.js') }}"> </script>
{% endblock %}